\newpage
\maketitle
\begin{center}
\Large \textbf{第Z01章 深度学习框架} \quad 
\end{center}
\begin{abstract}
在本章中我们将利用Numpy开发一个小型的深度学习框架，实现类似PyTorch的功能。
\end{abstract}
\section{深度学习框架概述}
在本章中，我们将利用Numpy，开发一个基于动态计算图的深度学习框架。
\subsection{张量}
在深度学习中，最基本的元素是张量（Tensor）。1维张量就是我们所熟悉的向量，2维张量就是矩阵，3维及以上就是通用的张量。张量（Tensor）在
Numpy中就用多维数组来表示。
我们首先定义一个仅支持加法运算，但是支持自动微分的张量原型，在后面的章节在逐渐扩充完善。
\subsubsection{最简张量模型}
我们首先定义一个最简单的张量，如下所示：
\lstset{language=PYTHON, caption={策略迭代(chpZ01/tensor.py)}, label={chpZ01-simple-tensor}}
\begin{lstlisting}
import numpy as np

class Tensor(object):
	def __init__(self, data, autograd=False, creators=None, creation_op=None, cid=None):
		self.data =np.array(data)
		self.creators = creators
		self.creation_op = creation_op
		self.autograd = autograd
		self.grad = None
		self.children = {}
		if (cid is None):
			cid = np.random.randint(0, 10000000)
		self.cid = cid
		if creators is not None:
			for c in creators:
				if self.cid not in c.children:
					c.children[self.cid] = 1
				else:
					c.children[self.cid] += 1

	def all_children_grads_accounted_for(self):
		for cid, cnt in self.children.items():
			if cnt != 0:
				return False
		return True

	def backward(self, grad=None, grad_origin=None):
		if self.autograd:
			if grad_origin is not None:
				if self.children[grad_origin.cid] == 0:
					raise Exception('cannot backprop more than once')
				else:
					self.children[grad_origin.cid] -= 1
			if self.grad is None:
				self.grad = grad
			else:
				self.grad += grad
			if self.creators is not None and (self.all_children_grads_accounted_for() or grad_origin is None):
				if self.creation_op == 'add':
					self.creators[0].backward(self.grad, self)
					self.creators[1].backward(self.grad, self)

	def __add__(self, other):
		if self.autograd and other.autograd:
			return Tensor(self.data + other.data, autograd=True, creators=[self, other], creation_op='add')
		return Tensor(self.data + other.data)

	def __repr__(self):
		return str(self.data.__repr__())

	def __str__(self):
		return str(self.data.__str__())

	def to_string(self):
		ts = 'tensor_{0}:\r\n'.format(self.cid)
		ts = '{0}    data:{1};\r\n'.format(ts, self.data)
		ts = '{0}    autograd: {1};\r\n'.format(ts, self.autograd)
		ts = '{0}    creators: {1};\r\n'.format(ts, self.creators)
		ts = '{0}    creation_op: {1};\r\n'.format(ts, self.creation_op)
		ts = '{0}    cid: {1};\r\n'.format(ts, self.cid)
		ts = '{0}    grad: {1};\r\n'.format(ts, self.grad)
		ts = '{0}    children: {1};\r\n'.format(ts, self.children)
		return ts
\end{lstlisting}
直接看代码比较难以理解，下面我们先写一个单元测试用例，实现一个前向传播过程，如下所示：
\lstset{language=PYTHON, caption={向量前向传播测试用例(chpZ01/tensor.py)}, label={chpZ01-simple-tensor-forward-unit-test001}}
\begin{lstlisting}
class TTensor(unittest.TestCase):
    @classmethod
    def setUp(cls):
        pass

    @classmethod
    def tearDown(cls):
        pass

    def test_init_001(self):
        a = Tensor([1, 2, 3, 4, 5], autograd=True)
        b = Tensor([10, 10, 10, 10, 10], autograd=True)
        c = Tensor([5, 4, 3, 2, 1], autograd=True)
        d = a + b
        e = b + c
        f = d + e
        print('f: {0};'.format(f.to_string()))
        print('d: {0};'.format(d.to_string()))
        print('e: {0};'.format(e.to_string()))
        print('a: {0};'.format(a.to_string()))
        print('b: {0};'.format(b.to_string()))
        print('c: {0};'.format(c.to_string()))
\end{lstlisting}
运行上面的测试用例：
\lstset{language=BASH, caption={向量前向传播测试用例运行(chpZ01/tensor.py)}, label={chpZ01-simple-tensor-forward-unit-test001-run}}
\begin{lstlisting}
python -m unittest uts.apps.drl.chpZ01.t_tensor.TTensor.test_init_001
\end{lstlisting}
运行结果如下所示：
\begin{figure}[H]
	\caption{向量前向传播执行结果}
	\label{p000015}
	\centering
	\includegraphics[height=10cm]{images/p000015}
\end{figure}
其将形成如下所示的动态图：
\begin{figure}[H]
	\caption{向量加法动态图示例}
	\label{p000016}
	\centering
	\includegraphics[height=10cm]{images/p000016}
\end{figure}
接下来我们看一下反向传播过程，首先来看测试用例：
\lstset{language=PYTHON, caption={向量前向传播测试用例(chpZ01/tensor.py)}, label={chpZ01-simple-tensor-forward-unit-test001}}
\begin{lstlisting}
class TTensor(unittest.TestCase):

	def test_add_backward_001(self):
		a = Tensor([1, 2, 3, 4, 5], autograd=True)
		b = Tensor([10, 10, 10, 10, 10], autograd=True)
		c = Tensor([5, 4, 3, 2, 1], autograd=True)
		d = a + b
		e = b + c
		f = d + e
		f.backward(Tensor([1, 1, 1, 1, 1]))
		print('f: {0};'.format(f.to_string()))
		print('d: {0};'.format(d.to_string()))
		print('e: {0};'.format(e.to_string()))
		print('a: {0};'.format(a.to_string()))
		print('b: {0};'.format(b.to_string()))
		print('c: {0};'.format(c.to_string()))
\end{lstlisting}
我们首先来看理论分析，对于$f=d+e $，我们先定$\frac{\partial j}{\partial f}=[1, 1, 1, 1, 1]$，根据链式求导法则，得到$\frac{\partial j}{\partial \boldsymbol{e}}=\frac{\partial j}{\partial \boldsymbol{f}} \frac{\partial \boldsymbol{f}}{\partial \boldsymbol{e}}$，
向量对向量微分是得到的是Jacobian矩阵，如下所示：
\begin{equation}
\begin{aligned}
\frac{\partial \boldsymbol{f}}{\partial \boldsymbol{e}} = \begin{bmatrix}
\frac{\partial f_{1}}{\partial e_{1}} & \frac{\partial f_{1}}{\partial e_{2}} & ... & \frac{\partial f_{1}}{\partial e_{n}} \\
\frac{\partial f_{2}}{\partial e_{1}} & \frac{\partial f_{2}}{\partial e_{2}} & ... & \frac{\partial f_{2}}{\partial e_{n}} \\
... & ... & ... & ... \\
\frac{\partial f_{n}}{\partial e_{1}} & \frac{\partial f_{n}}{\partial e_{2}} & ... & \frac{\partial f_{n}}{\partial e_{n}}
\end{bmatrix} \in R^{n \times n}
\end{aligned}
\label{chpZ01-vector-vector-gradient}
\end{equation}
其实际执行为$R^{1 \times n} \cdot R^{n \times n}=R^{1 \times n}$，以上为理论分析，下面我来看程序上面的代码实现。
对于向量$\boldsymbol{f}$，我们直接指定$\frac{\partial j}{\partial \boldsymbol{f}}=[1, 1, 1, 1, 1]$，向量$\boldsymbol{f}$当前状态为：
\begin{figure}[H]
	\caption{向量f原始状态}
	\label{p000017}
	\centering
	\includegraphics[height=5cm]{images/p000017}
\end{figure}
反向传播程序如下所示：
\lstset{language=PYTHON, caption={向量前向传播测试用例(chpZ01/tensor.py)}, label={chpZ01-simple-tensor-backward-f}}
\begin{lstlisting}
    def backward(self, grad=None, grad_origin=None):
        if self.autograd:
            if grad_origin is not None:
                if self.children[grad_origin.cid] == 0:
                    raise Exception('cannot backprop more than once')
                else:
                    self.children[grad_origin.cid] -= 1
            if self.grad is None:
                self.grad = grad
            else:
                self.grad += grad
            if self.creators is not None and (self.all_children_grads_accounted_for() or grad_origin is None):
                if self.creation_op == 'add':
                    self.creators[0].backward(self.grad, self)
                    self.creators[1].backward(self.grad, self)
\end{lstlisting}
代码解读如下所示：
\begin{itemize}
	\item 第2行：因为autograd为True，所以执行下面的代码；
	\item 第3$\sim$7行：因为grad\_origin为False，所以不执行；
	\item 第8$\sim$11行：因为grad为None，因此执行第9行使$grad=[1, 1, 1, 1, 1]$；
	\item 第12行：因为creators为d和e，且all\_children\_grads\_accounted\_for为True，且grad\_origin为None，因此会执行下面的语句；
	\item 第13$\sim$15行：由于是add运算，以自己的grad和自身为参数，分别调用d和e的后向传播算法；
\end{itemize}
执行完上述代码后，系统状态如下所示：
\begin{figure}[H]
	\caption{反向传播从f到d和e}
	\label{p000018}
	\centering
	\includegraphics[width=10cm]{images/p000018}
\end{figure}
我们先来看传到d节点，这时grad\_origin为f，此时d的children只有一个节点f，初始时children[f.cid]=1，运行后children[f.cid]=0，因为self.grad为空，
所以self.grad=[1, 1, 1, 1, 1]，因为self.creators=[a, b]，又为add操作，所以会调用a和b的反向传播算法。
\begin{figure}[H]
	\caption{反向传播从d到a和b}
	\label{p000019}
	\centering
	\includegraphics[width=10cm]{images/p000019}
\end{figure}
经过a和b的反向传播算法处理后，结果如下所示：
\begin{figure}[H]
	\caption{反向传播从d到a和b后的处理结果}
	\label{p000020}
	\centering
	\includegraphics[width=10cm]{images/p000020}
\end{figure}
我们再来看传到e节点，这时grad\_origin为f，此时d的children只有一个节点f，初始时children[f.cid]=1，运行后children[f.cid]=0，因为self.grad为空，
所以self.grad=[1, 1, 1, 1, 1]，因为self.creators=[a, b]，又为add操作，所以会调用b和c的反向传播算法。
\begin{figure}[H]
	\caption{反向传播从e到b和c}
	\label{p000021}
	\centering
	\includegraphics[width=10cm]{images/p000021}
\end{figure}
经过b和c的反向传播算法处理后，结果如下所示：
\begin{figure}[H]
	\caption{反向传播从e到b和c后的处理结果}
	\label{p000022}
	\centering
	\includegraphics[width=10cm]{images/p000022}
\end{figure}

